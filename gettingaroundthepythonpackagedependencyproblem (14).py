# -*- coding: utf-8 -*-
"""GettingAroundThePythonPackageDependencyProblem.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bp2fV1ssuBuVkTNfl-ryXTRdjoCMAVjV
"""

# -*- coding: utf-8 -*-
"""
Spyder Editor

This is a temporary script file.
"""

import math
import torch  #May 31st, 2025 update, added in PyTorch cuz https://tenor.com/search/spongebob-i-need-it-gifs



import json
import os
import numpy as np #very important to the sanity check in part 2

import pandas as pd
print(pd.__version__)

import tensorflow as tf
print(tf.__version__)

from transformers import TFBertModel


#from transformers import GPT2Tokenizer

#NEW DOCUMENTATION --- March 25th, 2025

#WE HAVE SWITCHED TO USING THE BERT TOKENIZER OF BERT BASED UNCASED
#GENERAL ADVISORY ---- REMEBER TO CONVERT IDS TO TOKENS, THEN TOKENS TO STRINGS




from transformers import BertTokenizer


# Load the tokenizer
#tokenizer = GPT2Tokenizer.from_pretrained("gpt2") #we may need to use bert instead
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = TFBertModel.from_pretrained("bert-base-uncased")  #Update as of 4/2/25



# Test tokenization
text = "Hello, TensorFlow!"
tokens = tokenizer.encode(text, return_tensors="tf")  # Use "tf" instead of "pt" for TensorFlow tensors

#print("Tokenized Output:", tokens)

#defines file name for multinli file in the C:\Users\swift\.spyder-py3 directory




def prep_data():
    print("d")

    #step 1 of the getting started

    #name of the multinli jsonl file
    file_name = "multinli_1.0_dev_matched.jsonl"

    #inorder to get only "sentence1, sentence2, and "gold_label" --- I must use keys to filter out dictionary data
    desired_keys = {"sentence1" , "sentence2" , "gold_label"}

    filtered_data = [] #place to store the filtered multinli data

    #getting the file open and reading it line by line with json package and not pandas
    with open(file_name, "r", encoding="utf-8") as file:
        for index, line in enumerate(file):
            nli_data = json.loads(line) #apparently this converts json files to python dictionary
            filtered_dict = {key:nli_data[key] for key in desired_keys if key in nli_data}
            filtered_dict["row number"] = index
            filtered_data.append(filtered_dict)

    #print(filtered_data)


    #step 2 --- Tokenize the data
    #can I return a tokenizer?
    vtokenizer = BertTokenizer.from_pretrained('bert-base-uncased')



    #tonkenizing just the data from sentence1 of the filtered data

    tokenized_s1 = []


    #Tokenized sentence 1 from the data
    for entry in filtered_data:
        sentence_from_data = entry["sentence1"]
        tokenized_s1.append(vtokenizer.encode(sentence_from_data, return_tensors="tf"))


    #print("Tokenized Output:" , tokenized_s1)

    tokenized_s2 = []  #apparently I did it wrong


    #Tokenized sentence 2 data
    for entry in filtered_data:
        sentence_from_data = entry["sentence2"]
        tokenized_s2.append(vtokenizer.encode(sentence_from_data, return_tensors = "tf"))

    #print("Tokenized Output:" , tokenized_s2)

    tokenized_gold = []


    #Tokenized gold_label data
    for entry in filtered_data:
        sentence_from_data = entry["gold_label"]
        tokenized_gold.append(vtokenizer.encode(sentence_from_data, return_tensors = "tf"))


    #print("Tokenized Output:" , tokenized_gold)



    #gonna write this stuff to file cuz I want it
    #These files are technially full of the token ids, not the tokens themselves

    with open("tokenized_s1.txt", "w", encoding= "utf-8") as file:
        file.write(str(tokenized_s1))

    with open("tokenized_s2.txt", "w", encoding= "utf-8") as file:
        file.write(str(tokenized_s2))

    with open("tokenized_gold.txt", "w", encoding= "utf-8") as file:
        file.write(str(tokenized_gold))




    #print(vtokenizer.decode(tokenized_s1))
    print("oe")

    #print(type(tokenized_s1))
    #print(type(tokenized_s1[0]))


    #https://chatgpt.com/c/67e37cd4-3e04-8009-8bba-c954c7e37e18 (Straight up didn't know that)

    # ________________________________________________

    # as a summary, basically tokenized_s1 is a list of 'tensorflow.python.framework.ops.EagerTensor's
    # But convert_ids_to_tokens expects a standard list of integers.
    # Hence why the below code does not work.

    '''   for i in tokenized_s1:
          print(vtokenizer.convert_tokens_to_string(vtokenizer.convert_ids_to_tokens(i)))'''

    #Therefore, we must instead take the iterater variable and do two steps
    #first we should do a .numpy() to convert it to a numpy arrray
    #second, while numpy arrays are not useful to us, we can then converst that to a regular list
    #via using .tolist()
    #this giving us a list of lists, which we can work with instead
    #the question is if I should change the data to be stored as a list of lists within the variables of
    # tokenized_s1 and tokenized_s2 themselves????????




    #Actually this is wrong!!!!!!

    #There was so much everything was just so wrong about it went way over my head with data types I didn't know even existed

    #____________________________________________________


    #reconstructed_strings = vtokenizer.batch_decode(tokenized_s1, skip_special_tokens=True)
    #print(reconstructed_strings)

        # Step 1: Inspect the structure of tokenized_s1
    print("Type of tokenized_s1:", type(tokenized_s1))  # Check the overall type
    print("First item in tokenized_s1:", tokenized_s1[0])  # Inspect the first item

    # Step 2: Convert tensors to lists of token IDs
    tokenized_s1_list = []


    # did not know flattening lists was a thing  --- taken from GPT
    for item in tokenized_s1:
        if isinstance(item, tf.Tensor):  # If it's a tensor, flatten and convert to a list
            tokenized_s1_list.append(item.numpy().flatten().tolist())
        else:
            print("Unexpected item type:", type(item))

    # Step 3: Use batch_decode with the correctly formatted list
    reconstructed_strings = vtokenizer.batch_decode(tokenized_s1_list, skip_special_tokens=True)
    print(reconstructed_strings)



    '''
    print(tf.__version__)


       '''


       #print(vtokenizer.convert_tokens_to_string(tokens))


    return tokenized_s1, tokenized_s2, tokenized_gold, vtokenizer



def step_3(tokenized_s1, tokenized_s2, tokenized_gold, vtokenizer):
    print('p')

    #temporarily holding onto data cuz I don't wanna lose it

    temptoke1 = tokenized_s1
    temptoke2 = tokenized_s2
    temptokeg = tokenized_gold


    what_the = vtokenizer.encode("the")
    print("encode", what_the )

    #THIS IS A SANITY CHECK TO MAKE SURE THAT WE CAN FIND OUT IF THE ENCODED ID FOR "the" IS TRUE
    print(vtokenizer.convert_tokens_to_string(vtokenizer.convert_ids_to_tokens(what_the)))

    #remember this is all just for removing "above"


    #we must first find the token ID for all those prepositions if we wish to remove them/mask them
    vector_rep_of_the_word_above = vtokenizer.encode("above")
    print("Encode:", vector_rep_of_the_word_above)


    #Sanity test for the encoded id of the word "above"

    print((vtokenizer.convert_tokens_to_string(vtokenizer.convert_ids_to_tokens(vector_rep_of_the_word_above))))


    #we need to check if the tokenizer is subtokenizing words or not
    i_am_a_lemon = vtokenizer.tokenize("above")
    print("Da Token!!!!!!: ", i_am_a_lemon)


    #looks like it is printing as just one token (no subtoken) [March 25 --- No longer case as we are now using Bert (delete comment)]

    #___________________________________________________

    '''
    #this should be most of the single word preositions I want to look for
    preposition_names = ["above", "below" , "behind" , "inside" , "outside"] #  'in front' and 'on top' currently excluded because they are multiple words; ie: multiple tokens
    tokenIDs_prepositions = []

    #looks for each preposition in preposition name, I had to correct my code with gpt2 as I was origianlly trying to iterated with [i] instead of .append

    for preposition in preposition_names:
        tokenIDs_prepositions.append(vtokenizer.encode(preposition))

    print(tokenIDs_prepositions)
    print(len(tokenIDs_prepositions))
    print(len(preposition_names))




    #gonna try and write these to a file now
    with open("tokenized_preposition_names.txt", "w", encoding= "utf-8") as file:
        array_length = len(tokenIDs_prepositions)
        for i in range (array_length):
            file.write("\n" + preposition_names[i] + ", TokenID: " +  str(tokenIDs_prepositions[i]))
    '''


    #print(tokenIDs_prepositions)


    #The file tokenized_preposition_names.txt" now contains the tokenIDs




    '''
    print("Type of TokenIDS" , type(tokenIDs_prepositions))

    for token_list in tokenIDs_prepositions:
       for token_id in token_list:
           token_id = int(token_id)
           print("Type of tokenized_s1:", type(tokenized_s1))
           print("Type of tokenized_s1:", type(tokenized_s1))
           toks1_list = tokenized_s1.tolist() if isinstance(tokenized_s1, np.ndarray) else tokenized_s1
           print("Type of tokss1_list:", type(toks1_list))
           exists = any(token_id == num for tup in toks1_list for num in tup) #apparently it is a tuple, so can't use ==
           print(f"Token ID {token_id} exists in batch: {exists}")

    '''

    #i just leared that BERT has these things called CLS and SEP, idk what it is but it is annoying and I want it gone
    #some sort of text wrapper or something

    #May 10 Update, I actually need those CLS and SEP tokens! I have realized!



















    #what this is doing here is looking for all the instances of the prespositions in each sentence in tokenized_s1
    #this just aint gonna work, i have done this for two days and gotten nowhere

    '''
    print("Type of TokenIDS:", type(tokenIDs_prepositions))

    for token_list in tokenIDs_prepositions:
        for token_id in token_list:
            token_id = int(token_id)  # Ensure token_id is an integer

            # Debugging the type of tokenized_s1
            print("Type of tokenized_s1:", type(tokenized_s1))

            # Convert tokenized_s1 to a list if it's a NumPy array
            toks1_list = tokenized_s1.tolist() if isinstance(tokenized_s1, np.ndarray) else tokenized_s1

            #get rid of the CLS and SEP



            # Debugging the type of the list
            #print("Type of toks1_list:", type(toks1_list))
            #print("Sample of tokenized_s1:", toks1_list[:5])


            # Check if token_id exists in any of the tuples inside tokenized_s1
            #exists = any(token_id == int(num.item()) if isinstance(num, np.ndarray) else token_id == num for tup in toks1_list for num in tup)

            exists = any(
                token_id == num if isinstance(num, np.int64) else np.array_equal(num, token_id)
                for tup in toks1_list
                for num in tup
            )

            if exists == True:
                print(f"Token ID {token_id} exists in batch: {exists}")
    '''
    '''
        above, TokenID: [101, 2682, 102]
        below, TokenID: [101, 2917, 102]
        behind, TokenID: [101, 2369, 102]
        inside, TokenID: [101, 2503, 102]
        outside, TokenID: [101, 2648, 102]


    '''
    '''
    cls_id = vtokenizer.cls_token_id
    sep_id = vtokenizer.sep_token_id


    above_tok = vtokenizer.convert_ids_to_tokens(vtokenizer.encode("above"))



    #Filter out special tokens from above_tok
    above_tok_filtered = [token for token in above_tok if token not in [cls_id, sep_id]]


    toks1_list = tokenized_s1.tolist() if isinstance(tokenized_s1, np.ndarray) else tokenized_s1
    toks1_list_filtered = [token for token in toks1_list if token not in [cls_id, sep_id]]

    #just give me the stats for above
    for i in range(len(toks1_list_filtered) - len(above_tok_filtered) + 1):  # Ensure no out-of-bound errors
        if toks1_list_filtered[i:i + len(above_tok_filtered)] == above_tok_filtered:
            print("reek")
    '''


    '''
    # Assuming you already have your tokenizer `vtokenizer`
    cls_id = vtokenizer.cls_token_id
    sep_id = vtokenizer.sep_token_id

    # Get the token IDs for "above"
    above_tok_ids = vtokenizer.encode("above")

    # Filter out special tokens from above_tok_ids (e.g., cls and sep)
    above_tok_ids_filtered = [token for token in above_tok_ids if token not in [cls_id, sep_id]]

    # Convert tokenized_s1 to a list if it's a NumPy array
    toks1_list = tokenized_s1.tolist() if isinstance(tokenized_s1, np.ndarray) else tokenized_s1

    # Filter out special tokens from toks1_list as well
    toks1_list_filtered = [token for token in toks1_list if token not in [cls_id, sep_id]]

    # Iterate through toks1_list_filtered to check if any subsequence matches above_tok_ids_filtered
    for i in range(len(toks1_list_filtered) - len(above_tok_ids_filtered) + 1):
        # Convert slice to numpy array for comparison
        if np.array_equal(np.array(toks1_list_filtered[i:i + len(above_tok_ids_filtered)]), above_tok_ids_filtered):
            print("Match found!")


    '''
    #-------------------- THIS IS ALL JUST FOR TOKENIZED_S1
    # 1. Define prepositions and get their token IDs (without [CLS]/[SEP])
    preposition_names = ["above", "below", "behind", "inside", "outside"]
    prep_token_ids = {
        prep: vtokenizer.encode(prep, add_special_tokens=False)[0]  # Get raw ID without [CLS]/[SEP]
        for prep in preposition_names
    }
    print("Preposition Token IDs:", prep_token_ids)

    # 2. Convert tokenized_s1 to a list of Python lists (filtering [CLS]/[SEP])
    #May 19th Update, CLS/SEP no longer being filtered (hope this doesn't break the code)
    s1_clean = []
    for tensor in tokenized_s1:
        token_ids = tensor.numpy().flatten().tolist()
        # Remove [CLS] (101) and [SEP] (102)
        #clean_ids = [tid for tid in token_ids if tid not in [101, 102]] //for getting rid of CLS & SEP Tokens
        clean_ids = [tid for tid in token_ids]
        s1_clean.append(clean_ids)


    preps_found_counter = 0  #this should count the # of preps found
    sentnum_s1 = [] # this array saves a list of sentence numbers that have those prepositions

    '''
    note that "above" should appear 30 times for s1
    24 for below in s1
    48 for behind
    42 for inside
    and 39 for outside

    with a total of 183 (so the counter should be around that number. )

    '''

    s1_clean_only_prep_sentences = [] #should only be the s1 sentences that have preps
    sentnum_s1_combined = [] #sentnum_s1_specific_prep combined (should be in same order as s1_clean_only_prep_sentences and all things derived from it as it is created as we iterate though the loop the same way)

    # 3. Find sentences containing each preposition (this is all just for s1)
    for prep, tid in prep_token_ids.items():
        print(f"\nSentences containing '{prep}':")
        sentnum_s1_specific_prep = [] #for a specific prep
        for i, sentence_ids in enumerate(s1_clean):
            if tid in sentence_ids:
                # Find position of the preposition
                pos = sentence_ids.index(tid)
                # Get context (3 words before/after)
                context_start = max(0, pos - 3)
                context_end = min(len(sentence_ids), pos + 4)
                context = sentence_ids[context_start:context_end]


                preps_found_counter += 1
                sentnum_s1.append(i)  #this should hopefully append the sentences that preps appear in
                sentnum_s1_specific_prep.append(i) #this should be for just one specific prep

                print(f"Sentence {i}:")
                print("Raw IDs:", context)
                print("Text:", vtokenizer.decode(context))


                s1_clean_only_prep_sentences.append(sentence_ids) #should be in same order as sentnum_s1_combined

        sentnum_s1_combined.append(sentnum_s1_specific_prep)

        print(f"\nNumbers of sentences containing '{prep}:", sentnum_s1_specific_prep)

        with open("Sentence_numbers_for_each_s1_that_contains_a_preposition.txt", "w", encoding= "utf-8") as file:
            file.write(f"\nNumbers of sentences containing '{prep}:" +  str(sentnum_s1_specific_prep))


    print("total number of those five preps found in s1 is : ", preps_found_counter)
    print("also sentnum_s1 is: ", sentnum_s1)

    with open("Sentence_numbers_for_each_s1_that_contains_a_preposition.txt", "w", encoding= "utf-8") as file:
        file.write("\n\ntotal number of those five preps found in s1 is : " +  str(preps_found_counter))

    print(sentnum_s1_combined)


    #so the total calculated form the pres_found_counter is 177
    #so there are sentences that are repeated.
    #makes me wonder if that may have caused the i variable that tells the sentence numbers if off
    #omg it is not off. https://docs.google.com/document/d/141nuHbimrYvBhI1CKSn8kkYTqjVMom4igF-z8u5WM84/edit?usp=sharing


    print(s1_clean_only_prep_sentences)


    for i, sentencewah in enumerate(s1_clean_only_prep_sentences):
        print(vtokenizer.convert_tokens_to_string(vtokenizer.convert_ids_to_tokens(sentencewah)))


    print(type(s1_clean_only_prep_sentences))
    print(type(s1_clean_only_prep_sentences[0]))

    print(type(tokenized_s1))
    print(type(tokenized_s1[0]))
    #s1_clean_only_prep_sentences is the list of list with only the prep sentences
    #tokenized_s1 is a list of all the s1 sentences as tensors

    #so I basically need to get the s1_clean_only_prep_sentences back to being a list of tensors
    #then I need to make sure that those tokens are matching the tokens of the originals in
    #tokenized_s1 --- because I don't want the data to be poisoned
    #then we need to get the vector reps of both with and without preps (masked and deleted)










    preps_ids_to_remove = [2682, 2917, 2369, 2503, 2648]

    # tokenized version of  s1_clean_only_prep_sentences will be called ts1p



    ts1p = [tf.convert_to_tensor(sentence, dtype=tf.int32) for sentence in s1_clean_only_prep_sentences]
    #remember that this is a list of lists



    print(ts1p)
    print(type(ts1p))
    print(type(ts1p[0]))



    #the process of removing those prepositions in s1

    ts1p_no_preps = [tf.convert_to_tensor([part for part in sentence.numpy() if part not in preps_ids_to_remove], dtype=tf.int32) for sentence in ts1p]

    print(type(ts1p_no_preps))   #list
    print(type(ts1p_no_preps[0]))  #tensorflow tensor


    print("This ts1p no preps: " , ts1p_no_preps[0])
    print(vtokenizer.convert_tokens_to_string(vtokenizer.convert_ids_to_tokens(ts1p_no_preps[0])))
    print("this is ts1p: ",  ts1p[0])
    print(vtokenizer.convert_tokens_to_string(vtokenizer.convert_ids_to_tokens(ts1p[0])))

    print(sentnum_s1_combined) #is also a list of lists


    #just gonna get the last hidden lay for ts1p_no_preps and ts1p

    #gonna pad them both


    #padding necesitates retokenization
    ts1p_padded = tf.keras.preprocessing.sequence.pad_sequences(ts1p, padding="post")
    ts1p_no_preps_padded = tf.keras.preprocessing.sequence.pad_sequences(ts1p_no_preps, padding = "post") #the weird padding thing, were 0 is the padding, idk if I want this

    #inputs ready for bert
    ts1p_pad_tensor  = tf.convert_to_tensor(ts1p_padded)
    ts1p_no_prep_pad_tensor = tf.convert_to_tensor(ts1p_no_preps_padded)


    #the big ticket money item here

    ts1p_outputs = bert_model(ts1p_pad_tensor)
    ts1p_no_prep_outputs = bert_model(ts1p_no_prep_pad_tensor)


    last_hid_state_ts1p = ts1p_outputs.last_hidden_state

    last_hid_state_ts1p_no_preps = ts1p_no_prep_outputs.last_hidden_state



    print("\n shape last hidden state of ts1p:    \n")



    print(last_hid_state_ts1p.shape)
    #print(ts1p_cls_head.last_hidden_state)

    print("\n shape of last hidden state of ts1p no prepositions: \n ")

    print(last_hid_state_ts1p_no_preps.shape)
    #print(ts1p_cls_head.last_hidden_state)


    print("\n__________________________________________")
    print("\n__________________________________________")


    #REACTIVATE THIS BLOCK OF CODE ONCE YOU GET THE CLS AND SEP TOKENS AGAIN.



    #print(last_hid_state_ts1p)
    #print(last_hid_state_ts1p_no_preps)


    with open("last_hid_state_ts1p.txt", "w", encoding = "utf-8") as file:
        file.write(str(last_hid_state_ts1p))

    with open("last_hid_state_ts1p_no_preps.txt", "w", encoding = "utf-8") as file:
        file.write(str(last_hid_state_ts1p_no_preps))

    print("\n__________________________________________")
    print("\n__________________________________________")
    print("\n__________________________________________")

    #_____________________________________________________-

    # __________ MAY 20th UPDATE::: Getting muh CLS tokens only!
    # https://www.youtube.com/watch?v=5PIM3cVcCqw <-- Great soundtrack for coding

    cls_embed_ts1p = last_hid_state_ts1p[:,0,:] #should just give the embedding of the CLS Token
    cls_embed_ts1p_no_preps = last_hid_state_ts1p_no_preps[:,0,:]






    '''
    for i, embedding in enumerate(last_hid_state_ts1p[0]):
        print(f"Token {i} embedding: {embedding}")
    '''

    #going to try and put text embeddings into each file itself

    for sent_index, sent_embeds in enumerate (last_hid_state_ts1p):
      file_name = f"last_hid_state_ts1p_sentence_embeddings[{sentnum_s1[sent_index]}].txt"
      print(sent_embeds.shape)
      with open (file_name, "w", encoding = "utf-8") as file:
        file.write(f"This is sentence number {sentnum_s1[sent_index]} in the original JSONL file\n")
        for tok_index, tok_embed in enumerate(sent_embeds):
          file.write(f"Token {tok_index} embedding: {tok_embed.numpy().tolist()}\n")


    #again but now for the no-preps

    for sent_index, sent_embeds in enumerate (last_hid_state_ts1p_no_preps):
      file_name = f"NO_PREPOSITIONS_last_hid_state_ts1p_sentence_embeddings[{sentnum_s1[sent_index]}].txt"
      print(sent_embeds.shape)
      with open (file_name, "w", encoding = "utf-8") as file:
        file.write(f"This is sentence number {sentnum_s1[sent_index]} in the original JSONL file\n")
        for tok_index, tok_embed in enumerate(sent_embeds):
          file.write(f"Token {tok_index} embedding: {tok_embed.numpy().tolist()}\n")




    #again but now for just the CLS tokens!

    with open ("cls_embed_ts1p.txt", "w", encoding = "utf-8") as file:
      for index, the_embedding in enumerate(cls_embed_ts1p):  #I HATE TUPLES SO MUCH AAAAAAAAAAAAAAAAAAAAAHHHHHHHHHHHHHHHHH!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
        file.write(f"This is the CLS embedding for sentence number {sentnum_s1[index]} in the original JSON file: \n")
        file.write(f"CLS embedding: {the_embedding.numpy().tolist()} \n\n")


    with open ("cls_embed_ts1p_no_preps.txt", "w", encoding = "utf-8") as file:
      for index, the_embedding in enumerate(cls_embed_ts1p_no_preps):
        file.write(f"This is the CLS embedding for sentence number {sentnum_s1[index]} in the original JSON file: \n")
        file.write(f"CLS embedding: {the_embedding.numpy().tolist()} \n\n")


    print(sentnum_s1)

    return cls_embed_ts1p, cls_embed_ts1p_no_preps, sentnum_s1









    '''






    with open("last_hid_state_ts1p_embeddings[0].txt" ,"w", encoding = "utf-8") as file:
        for i, embedding in enumerate(last_hid_state_ts1p[0]):
            file.write(f"Token {i} embedding: {embedding}\n")



    '''



    #__________________________________________________________________
    '''
    input_tensor = tf.convert_to_tensor(ts1p_padded)

    # Pass through BERT to get hidden states
    outputs = bert_model(input_tensor)

    outputs = bert_model(input_tensor)

    last_hidden_states = outputs.last_hidden_state

    print("look here" , last_hidden_states.shape)

    '''

    '''









    #worry about this later
    with open("sentence_s1_only_preps_without_preps_whole_shabang.txt", "w", encoding = "utf-8") as file:
        for i in range(len(ts1p_no_preps)):
            for j in range(len(sentnum_s1_combined)):
                for k in range(len(sentnum_s1_combined[j])):
                    file.write("\n This is sentence #" + str(sentnum_s1_combined[j][k]) + " from the s1 data set with its prepositions removed")



    #fudge! They don't have the same length
    if len(sentnum_s1_combined) == len(ts1p_no_preps):
        print("true")
    else:
        print("false")
    '''



    #[2682, 2917, 2369, 2503, 2648]: #this is a library of the five token ids of above, below, behind, inside, and outside
    '''
    for i in ts1p:
        for j in i:
            if j not in [2682, 2917, 2369, 2503, 2648]:
                ts1p_no_preps = []


    '''






    #______________________________________________________


    '''
    #now we are going to try and rip that vector rep out of tokenized_s1

    for i, sentence_tokens in enumerate(tokenized_s1):
        sentence_tokens = [token for token in sentence_tokens != vector_rep_of_the_word_above]

        #this should hopefully iterate through each setnence in sentence 1 and remove "above" via token ID
        tokenized_s1[i] = sentence_tokens


    with open("tokenized_s1_no_above.txt", "w", encoding= "utf-8") as file:
        file.write(str(tokenized_s1))


    #print(tokenized_s1) #apparently this is tokenized_s1 without "above" now

    #now I need to get the vector reps of it

    #get outputs of tokenized
    #get embeddings from the outputs
    #get last hidden layer from embeddings (is this the last feedforward? )
    #had to ask GPT about how to do those 3 steps. I was under the convention I could just ask for tokenized.vector-reps(). turns out that is not true
     '''

    #need to get each sentence with each preposition

    #gonna first try to flatten the tensor to a list (making it an array of arrays),
    #then look for the token id of the prepositions,
    #and then save only those arrays that have those preposition ids

    '''
    #flattening s1
    tokenized_s1_flattened = []


    for item in tokenized_s1:
        if isinstance(item, tf.Tensor):  # If it's a tensor, flatten and convert to a list
            tokenized_s1_flattened.append(item.numpy().flatten().tolist())
        else:
            print("Unexpected item type:", type(item))



    print(type(tokenized_s1_flattened))

    print(len(tokenized_s1_flattened))


    #lenght_flat_s1 = len(tokenized_s1_flattened)

    #finds the preps (just above as test trial)

    #remember that tokenIDs_prepositions preposition_names got the same length

    tokenized_s1_only_preps_flattened = []
    '''
    '''
    for p in preposition_names:
        for a_sentence in tokenized_s1_flattened:
            if tokenIDs_prepositions[p] is in tokenized_s1_flattened[a_sentence]:
                tokenized_s1_only_preps_flattened.append(tokenized_s1_flattened[a_sentence])
    '''


    '''
    #both are important for labelling what sentence is what in the file "tokenized_s1_only_preps_flattened.txt"
    sentence_number_iterator = 0
    sentence_number_tracker = []

    for p in preposition_names:    #stole this from gpt after trying to implement ^^^
        preposition_tokens = tokenIDs_prepositions[preposition_names.index(p)]  # Get the token IDs for the preposition
        for a_sentence in tokenized_s1_flattened:
            # Check if any token in the sentence matches the preposition's token
            sentence_number_iterator += 1 #is of original design
            if any(token in a_sentence for token in preposition_tokens):
                tokenized_s1_only_preps_flattened.append(a_sentence)
                sentence_number_tracker.append(sentence_number_iterator) #---this line is of original design (supposed to store only iterated numbers  of sentences with preps, and skip other numbers in the w/o prep sentences)
                break #this prevents sentences with multiple prepositions being counted more than once


    #if any(token in a_sentence for token in preposition_tokens):
    #The worst thing for me about python is that i would have never guessed how to
    #write that one line so simply in python, I still have the C++ mentality of trying
    #to make that happen with nested forloops


    #remember that sentence_number_tracker and tokenized_s1_only_preps_flattened should have the same length

    print(tokenized_s1_only_preps_flattened)
    print(sentence_number_tracker)

    with open("tokenized_s1_only_preps_flattened.txt", "w", encoding= "utf-8") as file:
        lengthy_boi = len(tokenized_s1_only_preps_flattened)
        for i in range (lengthy_boi):
            file.write("\n A Preposition on the list appears in Sentence Number: " + str(sentence_number_tracker[i]) + str(tokenized_s1_only_preps_flattened[i]) + "\n")

    '''


def step_4(tokenized_s2, vtokenizer):

    #-------------------- THIS IS ALL JUST FOR TOKENIZED_S2
    # 1. Define prepositions and get their token IDs (without [CLS]/[SEP])
    preposition_names = ["above", "below", "behind", "inside", "outside"]
    prep_token_ids = {
        prep: vtokenizer.encode(prep, add_special_tokens=False)[0]  # Get raw ID without [CLS]/[SEP]
        for prep in preposition_names
    }
    print("Preposition Token IDs:", prep_token_ids)

    # 2. Convert tokenized_s2 to a list of Python lists (filtering [CLS]/[SEP])
    #May 19th Update, CLS/SEP no longer being filtered (hope this doesn't break the code)
    s2_clean = []
    for tensor in tokenized_s2:
        token_ids = tensor.numpy().flatten().tolist()
        # Remove [CLS] (101) and [SEP] (102)
        #clean_ids = [tid for tid in token_ids if tid not in [101, 102]] //for getting rid of CLS & SEP Tokens
        clean_ids = [tid for tid in token_ids]
        s2_clean.append(clean_ids)


    preps_found_counter = 0  #this should count the # of preps found
    sentnum_s2 = [] # this array saves a list of sentence numbers that have those prepositions

    '''
    note that "above" should appear 11 times for s2
    10 for below in s2
    22 for behind
    26 for inside
    and 24 for outside

    with a total of 93 (so the counter should be around that number. )

    '''

    s2_clean_only_prep_sentences = [] #should only be the s2 sentences that have preps
    sentnum_s2_combined = [] #sentnum_s2_specific_prep combined (should be in same order as s2_clean_only_prep_sentences and all things derived from it as it is created as we iterate though the loop the same way)

    # 3. Find sentences containing each preposition (this is all just for s2)
    for prep, tid in prep_token_ids.items():
        print(f"\nSentences containing '{prep}':")
        sentnum_s2_specific_prep = [] #for a specific prep
        for i, sentence_ids in enumerate(s2_clean):
            if tid in sentence_ids:
                # Find position of the preposition
                pos = sentence_ids.index(tid)
                # Get context (3 words before/after)
                context_start = max(0, pos - 3)
                context_end = min(len(sentence_ids), pos + 4)
                context = sentence_ids[context_start:context_end]


                preps_found_counter += 1
                sentnum_s2.append(i)  #this should hopefully append the sentences that preps appear in
                sentnum_s2_specific_prep.append(i) #this should be for just one specific prep

                print(f"Sentence {i}:")
                print("Raw IDs:", context)
                print("Text:", vtokenizer.decode(context))


                s2_clean_only_prep_sentences.append(sentence_ids) #should be in same order as sentnum_s2_combined

        sentnum_s2_combined.append(sentnum_s2_specific_prep)

        print(f"\nNumbers of sentences containing '{prep}:", sentnum_s2_specific_prep)

        with open("Sentence_numbers_for_each_s2_that_contains_a_preposition.txt", "w", encoding= "utf-8") as file:
            file.write(f"\nNumbers of sentences containing '{prep}:" +  str(sentnum_s2_specific_prep))


    print("total number of those five preps found in s2 is : ", preps_found_counter)
    print("also sentnum_s2 is: ", sentnum_s2)

    with open("Sentence_numbers_for_each_s2_that_contains_a_preposition.txt", "w", encoding= "utf-8") as file:
        file.write("\n\ntotal number of those five preps found in s2 is : " +  str(preps_found_counter))

    print(sentnum_s2_combined)


    #so the total calculated form the pres_found_counter is 177
    #so there are sentences that are repeated.
    #makes me wonder if that may have caused the i variable that tells the sentence numbers if off
    #omg it is not off. https://docs.google.com/document/d/141nuHbimrYvBhI1CKSn8kkYTqjVMom4igF-z8u5WM84/edit?usp=sharing


    print(s2_clean_only_prep_sentences)


    for i, sentencewah in enumerate(s2_clean_only_prep_sentences):
        print(vtokenizer.convert_tokens_to_string(vtokenizer.convert_ids_to_tokens(sentencewah)))













    preps_ids_to_remove = [2682, 2917, 2369, 2503, 2648] #hopefully these are still the same

    # tokenized version of  s2_clean_only_prep_sentences will be called ts2p



    ts2p = [tf.convert_to_tensor(sentence, dtype=tf.int32) for sentence in s2_clean_only_prep_sentences]
    #remember that this is a list of lists



    print(ts2p)
    print(type(ts2p))
    print(type(ts2p[0]))



    #the process of removing those prepositions in s2

    ts2p_no_preps = [tf.convert_to_tensor([part for part in sentence.numpy() if part not in preps_ids_to_remove], dtype=tf.int32) for sentence in ts2p]

    print(type(ts2p_no_preps))   #list
    print(type(ts2p_no_preps[0]))  #tensorflow tensor


    print("This ts2p no preps: " , ts2p_no_preps[0])
    print(vtokenizer.convert_tokens_to_string(vtokenizer.convert_ids_to_tokens(ts2p_no_preps[0])))
    print("this is ts2p: ",  ts2p[0])
    print(vtokenizer.convert_tokens_to_string(vtokenizer.convert_ids_to_tokens(ts2p[0])))

    print(sentnum_s2_combined) #is also a list of lists


    #just gonna get the last hidden lay for ts2p_no_preps and ts2p

    #gonna pad them both


    #padding necesitates retokenization
    ts2p_padded = tf.keras.preprocessing.sequence.pad_sequences(ts2p, padding="post")
    ts2p_no_preps_padded = tf.keras.preprocessing.sequence.pad_sequences(ts2p_no_preps, padding = "post") #the weird padding thing, were 0 is the padding, idk if I want this

    #inputs ready for bert
    ts2p_pad_tensor  = tf.convert_to_tensor(ts2p_padded)
    ts2p_no_prep_pad_tensor = tf.convert_to_tensor(ts2p_no_preps_padded)


    #the big ticket money item here

    ts2p_outputs = bert_model(ts2p_pad_tensor)
    ts2p_no_prep_outputs = bert_model(ts2p_no_prep_pad_tensor)


    last_hid_state_ts2p = ts2p_outputs.last_hidden_state

    last_hid_state_ts2p_no_preps = ts2p_no_prep_outputs.last_hidden_state



    print("\n shape last hidden state of ts2p:    \n")



    print(last_hid_state_ts2p.shape)
    #print(ts2p_cls_head.last_hidden_state)

    print("\n shape of last hidden state of ts2p no prepositions: \n ")

    print(last_hid_state_ts2p_no_preps.shape)
    #print(ts2p_cls_head.last_hidden_state)


    print("\n__________________________________________")
    print("\n__________________________________________")


    #REACTIVATE THIS BLOCK OF CODE ONCE YOU GET THE CLS AND SEP TOKENS AGAIN.



    #print(last_hid_state_ts2p)
    #print(last_hid_state_ts2p_no_preps)


    with open("last_hid_state_ts2p.txt", "w", encoding = "utf-8") as file:
        file.write(str(last_hid_state_ts2p))

    with open("last_hid_state_ts2p_no_preps.txt", "w", encoding = "utf-8") as file:
        file.write(str(last_hid_state_ts2p_no_preps))

    print("\n__________________________________________")
    print("\n__________________________________________")
    print("\n__________________________________________")

    #_____________________________________________________-

    # __________ MAY 20th UPDATE::: Getting muh CLS tokens only!
    # https://www.youtube.com/watch?v=5PIM3cVcCqw <-- Great soundtrack for coding

    cls_embed_ts2p = last_hid_state_ts2p[:,0,:] #should just give the embedding of the CLS Token
    cls_embed_ts2p_no_preps = last_hid_state_ts2p_no_preps[:,0,:]








    #going to try and put text embeddings into each file itself

    for sent_index, sent_embeds in enumerate (last_hid_state_ts2p):
      file_name = f"last_hid_state_ts2p_sentence_embeddings[{sentnum_s2[sent_index]}].txt"
      print(sent_embeds.shape)
      with open (file_name, "w", encoding = "utf-8") as file:
        file.write(f"This is sentence number {sentnum_s2[sent_index]} in the original JSONL file\n")
        for tok_index, tok_embed in enumerate(sent_embeds):
          file.write(f"Token {tok_index} embedding: {tok_embed.numpy().tolist()}\n")


    #again but now for the no-preps

    for sent_index, sent_embeds in enumerate (last_hid_state_ts2p_no_preps):
      file_name = f"NO_PREPOSITIONS_last_hid_state_ts2p_sentence_embeddings[{sentnum_s2[sent_index]}].txt"
      print(sent_embeds.shape)
      with open (file_name, "w", encoding = "utf-8") as file:
        file.write(f"This is sentence number {sentnum_s2[sent_index]} in the original JSONL file\n")
        for tok_index, tok_embed in enumerate(sent_embeds):
          file.write(f"Token {tok_index} embedding: {tok_embed.numpy().tolist()}\n")




    #again but now for just the CLS tokens!

    with open ("cls_embed_ts2p.txt", "w", encoding = "utf-8") as file:
      for index, the_embedding in enumerate(cls_embed_ts2p):  #I HATE TUPLES SO MUCH AAAAAAAAAAAAAAAAAAAAAHHHHHHHHHHHHHHHHH!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
        file.write(f"This is the CLS embedding for sentence number {sentnum_s2[index]} in the original JSON file: \n")
        file.write(f"CLS embedding: {the_embedding.numpy().tolist()} \n\n")


    with open ("cls_embed_ts2p_no_preps.txt", "w", encoding = "utf-8") as file:
      for index, the_embedding in enumerate(cls_embed_ts2p_no_preps):
        file.write(f"This is the CLS embedding for sentence number {sentnum_s2[index]} in the original JSON file: \n")
        file.write(f"CLS embedding: {the_embedding.numpy().tolist()} \n\n")


    print(sentnum_s2)

    return cls_embed_ts2p, cls_embed_ts2p_no_preps, sentnum_s2





def vector_summarization_step(cls_embed_ts1p, cls_embed_ts1p_no_preps, sentnum_s1, cls_embed_ts2p, cls_embed_ts2p_no_preps, sentnum_s2):
    print("vector summarization step")


    #first step is to do mean pooling


    collapsed_cls_1 = np.mean(cls_embed_ts1p, axis=0)
    collapsed_cls_1_no_prep = np.mean(cls_embed_ts1p_no_preps, axis=0)


    print("\n++++++++++++++++++++++++++\n")
    print("\n Collapsed CLS Vector Embedding for cls_embed_1 \n ")
    print(collapsed_cls_1)
    print("\n Collapsed CLS Vector Embedding for cls_embed_1_no_prep \n ")
    print(collapsed_cls_1_no_prep)


    collapsed_cls_2 = np.mean(cls_embed_ts2p, axis=0)
    collapsed_cls_2_no_prep = np.mean(cls_embed_ts2p_no_preps, axis=0)


    print("\n++++++++++++++++++++++++++\n")
    print("\n Collapsed CLS Vector Embedding for cls_embed_2 \n ")
    print(collapsed_cls_2)
    print("\n Collapsed CLS Vector Embedding for cls_embed_1_no_prep \n ")
    print(collapsed_cls_2_no_prep)





    #step 1.5 (getting the shift)
    #this is supposed to get the geometrically represented semantic shift between sentence 1 set and sentence 2 set with and without preps
    #this is the magical, putting it together part, he was talkin about.

    shift_1_2 = collapsed_cls_1 - collapsed_cls_2

    print("\n ++++++++++++++++++++++++ \n ")
    print("\n shift between 1 and 2 with preps \n ")
    print(shift_1_2)

    shift_1_2_no_prep = collapsed_cls_1_no_prep - collapsed_cls_2_no_prep

    print("\n ++++++++++++++++++++++++ \n ")
    print("\n shift between 1 and 2 no preps \n ")

    shift_1_2_no_prep = collapsed_cls_1_no_prep - collapsed_cls_2_no_prep


    #second step is to do cosine similiarity

    #because i need to use pythorch, i need to convert my tensors to a format that can be used by Pytorch

    shift_torch_12 = torch.tensor(shift_1_2, dtype = torch.float32)
    shift_torch_12_no_prep = torch.tensor(shift_1_2_no_prep, dtype = torch.float32)


    #cosine similarity time

    cos_sim = torch.nn.functional.cosine_similarity(shift_torch_12.unsqueeze(0), shift_torch_12_no_prep.unsqueeze(0)).item()

    print("\n\n\n\n cos_sim: ")


    print(cos_sim)





















def main():
    tokenized_s1, tokenized_s2, tokenized_gold, vtokenizer = prep_data()
    cls_embed_ts1p, cls_embed_ts1p_no_preps, sentnum_s1 = step_3(tokenized_s1, tokenized_s2, tokenized_gold, vtokenizer)
    cls_embed_ts2p, cls_embed_ts2p_no_preps, sentnum_s2 = step_4(tokenized_s2, vtokenizer) #repeating step 3 but now with tokenized_s2




    vector_summarization_step(cls_embed_ts1p, cls_embed_ts1p_no_preps, sentnum_s1, cls_embed_ts2p, cls_embed_ts2p_no_preps, sentnum_s2)
    #vector_summarization_step should mean pool cls_embed_1, cls_embed_1_no_prep, cls_embed_2, and cls_embed_2_no_prep
    #it should then take the means of cls_embed_1 and cls_embed_1_no_prep and do a cosine similarity; likewise for sentence 2 data
    #then somehow calculate difference in distance between the cosine similarities 1 and 2 for both prep and no prep







main()